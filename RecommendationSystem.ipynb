{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecommendationSystem.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichekhovskikh/fasttext/blob/master/RecommendationSystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-AdmXNJwRcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pymorphy2[fast]\n",
        "\n",
        "import gensim\n",
        "import os\n",
        "import collections\n",
        "import smart_open\n",
        "import random\n",
        "import json\n",
        "import urllib.request\n",
        "import pymorphy2\n",
        "import nltk\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZYD5AXcNYRU",
        "colab_type": "text"
      },
      "source": [
        "#Начинаем\n",
        "Для начала нам понадобится комплект документов для обучения нашей модели FastText. Теоретически, документ может быть чем угодно: коротким твитом из 140 символов, отдельным абзацем, новостной статьей или книгой. В NLP комплект документов часто называют корпусом.\n",
        "\n",
        "Будем тренировать нашу модель на собственном корпусе. Этот корпус содержит 500 научных статей на 5 различных тем.\n",
        "\n",
        "Для тестирования будет использоваться тестовый корпус из 50 статей (10 статей на каждую тему).\n",
        "\n",
        "Dataset состоит из трех строк: id (идентификатор строки), text (текст статьи), tag (идентификатор самой статьи, вектор которого будем обучать), class_name (был размечен вручную, необходим только для тестирования)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KR5mKmSVAc-L",
        "colab": {}
      },
      "source": [
        "#@title Введите путь к файлам исходной базы статей:\n",
        "train_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/train_corpus.json' #@param {type: \"string\"}\n",
        "test_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/test_corpus.json' #@param {type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzY-WMJjJhtR",
        "colab_type": "text"
      },
      "source": [
        "## Опредлим функцию для чтения и предварительной обработки текста\n",
        "Ниже мы определяем функцию для открытия  train/test файла, предварительно обрабатываем каждый текст датасета, используя простой инструмент предварительной обработки gensim (то есть, разбиваем текст на отдельные слова, удалите знаки препинания, установите строчные буквы и т. д.), лемматизацию, удаление стоп слов и возвращаем список слов. Для обучения модели нам нужно будет связать тег с каждым документом учебного корпуса. В нашем случае тег - это идентификатор статьи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwpMLNmwZ4v",
        "colab_type": "text"
      },
      "source": [
        "Лемматизация каждого слова статьи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO_znw317kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lemmatize(words):\n",
        "    for word in words:\n",
        "        yield morph.parse(word)[0].normal_form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr6Qz8Ucw_ee",
        "colab_type": "text"
      },
      "source": [
        "Удаление стоп слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRm8tmSe17Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    return [word for word in words if word not in russian_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB82kEYi-nek",
        "colab_type": "text"
      },
      "source": [
        "Предобработка текста статьи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwqCiuN-hDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def advanced_preprocess(text):\n",
        "    normalized_text = gensim.utils.simple_preprocess(text)\n",
        "    normalized_text = list(lemmatize(normalized_text))\n",
        "    normalized_text = remove_stopwords(normalized_text)\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agV69PVxEIl",
        "colab_type": "text"
      },
      "source": [
        "Отрытие файла с корпусом статей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMO4zHrqGTws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        for article in corpus:\n",
        "            normalized_text = advanced_preprocess(article['text'])\n",
        "            yield normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_nR03Khn6Lds"
      },
      "source": [
        "Получение исходного текста статьи по индексу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1gSBZ1G1BNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_text_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsDE0caoRrMN",
        "colab_type": "text"
      },
      "source": [
        "Получение категории статьи по индексу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMV7VB-Rn-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_class_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['class_name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3oWTnrxKNt",
        "colab_type": "text"
      },
      "source": [
        "Загружаем корпуса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0vJrTipGemC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_corpus = list(read_corpus(train_path))\n",
        "test_corpus = list(read_corpus(test_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKA3sFQxKYJP",
        "colab_type": "text"
      },
      "source": [
        "Давайте посмотрим на учебный и тестовый корпуса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aPeDPvGgve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29upY6eyKlmk",
        "colab_type": "text"
      },
      "source": [
        "Корпус тестирования выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWLt9Z8Gm66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-cTR8Uz7JYp",
        "colab_type": "text"
      },
      "source": [
        "# Обучение модели\n",
        "## Создание объекта FastText\n",
        "Теперь мы создадим модель FastText с векторным размером 80 слов и перебираем учебный корпус 100 раз.\n",
        "Параметры обучения:\n",
        "- size: размерность вектора\n",
        "- window: «окно», в котором рассматривается контекст\n",
        "- alpha: коэфициент обучения\n",
        "- sg: cbow (0) или skip-gram (1)\n",
        "- epochs: количество иттераций"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6w0x7RlnzVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Укажите параметры обучения модели:\n",
        "vector_size = 80 #@param\n",
        "window =  8 #@param\n",
        "epochs =  100 #@param\n",
        "alpha = 0.001 #@param\n",
        "learning_method = \"skip-gram\" #@param [\"skip-gram\", \"CBOW\"] {type:\"string\"}\n",
        "\n",
        "sg = 1\n",
        "if (learning_method == \"CBOW\"):\n",
        "    sg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPL0bxPW7RN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FastText(\n",
        "    size=vector_size, \n",
        "    window=window,\n",
        "    alpha=alpha,\n",
        "    sg=sg)\n",
        "model.build_vocab(sentences=train_corpus)\n",
        "%time model.train(sentences=train_corpus, total_examples=model.corpus_count, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcPTXq2eSNFS",
        "colab_type": "text"
      },
      "source": [
        "# Опредлим функции для преобразования вектора слов в вектор текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeuA40DoSi0x",
        "colab_type": "text"
      },
      "source": [
        "Обратная сглаженная частота (вес слова), где\n",
        "- a: гиперпараметр, который обычно равен 0.001 (конфигурация, внешняя по отношению к модели, значение которой невозможно оценить по данным):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCA8NOFtSZ0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = 0.001\n",
        "\n",
        "def sif(word, paragraph):\n",
        "    frequency = paragraph.count(word) / len(paragraph)\n",
        "    return a / (a + frequency) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyOLBTMAVVml",
        "colab_type": "text"
      },
      "source": [
        "Получить вектор текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp9jnjq1Zahc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def paragraph_vector(model, paragraph):\n",
        "    result_vector = []\n",
        "    for word in paragraph:\n",
        "        weight = sif(word, paragraph)\n",
        "        try:\n",
        "            word_vector = model.wv[word]\n",
        "            weighted_word_vector = [weight * elem for elem in word_vector]\n",
        "            if not result_vector:\n",
        "                result_vector = weighted_word_vector\n",
        "            else: \n",
        "                result_vector = [x + y for x, y in zip(result_vector, weighted_word_vector)]\n",
        "        except:\n",
        "            continue\n",
        "    return result_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbmpPBjkentu",
        "colab_type": "text"
      },
      "source": [
        "Вычислим вектора для всех статей исходной базы для ускорения работы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhs-1jl3exHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraph_vectors = []\n",
        "for text in train_corpus:\n",
        "    text_vector = paragraph_vector(model, text)\n",
        "    paragraph_vectors.append(text_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OriWNZ5RiUVd",
        "colab_type": "text"
      },
      "source": [
        "# Поиск похожих текстов на основе обученной модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "one34UYqheKH",
        "colab_type": "text"
      },
      "source": [
        "Определение косинусной близости двух векторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtIx6nhchqDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(first_vector, second_vector):\n",
        "    numerator = sum([x * y for x, y in zip(first_vector, second_vector)])\n",
        "    first_norm = pow(sum([x * x for x in first_vector]), 0.5)\n",
        "    second_norm = pow(sum([x * x for x in second_vector]), 0.5)\n",
        "    return numerator / (first_norm * second_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jak5jPaK6QmC",
        "colab_type": "text"
      },
      "source": [
        "Поиск похожих текстов из корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omwDc4Vd6YIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sims(model, paragraph, count = 5):\n",
        "    result = []\n",
        "    compared_vector = paragraph_vector(model, paragraph)\n",
        "    for index, text_vector in enumerate(paragraph_vectors):\n",
        "        similarity = cosine_similarity(compared_vector, text_vector)\n",
        "        result.append([index, similarity])\n",
        "    return sorted(result,  key=lambda x: x[1], reverse=True)[:count]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeGSybK4tpG",
        "colab_type": "text"
      },
      "source": [
        "# Оценочная модель\n",
        "Чтобы оценить нашу новую модель, мы сначала выведем новые векторы для каждого документа тренировочного корпуса, сравним выведенные векторы с тренировочным корпусом.\n",
        "\n",
        "Проверка выведенного вектора по обучающему вектору является своего рода «проверкой работоспособности» в отношении того, ведет ли модель себя адекватно, хотя и не является реальным значением «точности».\n",
        "\n",
        "Можем взглянуть на пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KendqFnt496K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "doc = train_corpus[doc_id]\n",
        "sims = sims(model, doc)\n",
        "print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}): «{}»\\n'.format(doc_id, get_article_text_by_index(doc_id, train_path)))\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, get_article_text_by_index(index, train_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eq8rXTHOwBQ",
        "colab_type": "text"
      },
      "source": [
        "# Тестирование модели\n",
        "Используя тот же подход, что и выше, мы выведем вектор для случайно выбранного тестового документа и сравним документ с нашей моделью на глаз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y7K6N4jLIXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "test_size = len(test_corpus)\n",
        "\n",
        "predicted_classes = [] \n",
        "test_classes = []\n",
        "\n",
        "for doc_index in range(test_size):\n",
        "    doc = test_corpus[doc_index]\n",
        "    similarities = sims(model, doc)\n",
        "    \n",
        "    test_article_class = get_article_class_by_index(doc_index, test_path)\n",
        "    print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}) «{}»: «{}»\\n'.format(doc_index, test_article_class, get_article_text_by_index(doc_index, test_path)))\n",
        "    num = 0\n",
        "    for index, similarity in similarities:\n",
        "        num += 1\n",
        "        predicted_article_class = get_article_class_by_index(index, train_path)\n",
        "        print(u'%s) %s «%s»: «%s»\\n' % (num, similarity, predicted_article_class, get_article_text_by_index(index, train_path)))\n",
        "        predicted_classes.append(predicted_article_class)\n",
        "        test_classes.append(test_article_class)\n",
        "        \n",
        "print('f1score: {}'.format(f1_score(test_classes, predicted_classes, average='macro')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91sOWCLiLOxS",
        "colab_type": "text"
      },
      "source": [
        "Тестирование на малых данных для выявления ошибок в ходе написания кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiPobEN6XScZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "\n",
        "print(common_texts)\n",
        "\n",
        "model = FastText(size=80, window=4, min_count=2, sample=1e-2, sg=1)\n",
        "model.build_vocab(sentences=common_texts)\n",
        "model.train(sentences=common_texts, total_examples=model.corpus_count, epochs=10)\n",
        "\n",
        "first_vector = paragraph_vector(model, ['human', 'interface', 'computer'])\n",
        "second_vector = paragraph_vector(model, ['human', 'user', 'interface', 'system'])\n",
        "print(cosine_similarity(first_vector, second_vector))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSpkRxWUrC9B"
      },
      "source": [
        "# Поиск похожих научных документов\n",
        "Выполните поиск похожих научных статей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrflInTzrgmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Укажите путь к тексту статьи в формате *.txt или введите текст статьи:\n",
        "article_text = '' #@param {type: \"string\"}\n",
        "article_path = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/labs/example1.txt' #@param {type: \"string\"}\n",
        "\n",
        "if (article_text == ''):\n",
        "    with urllib.request.urlopen(article_path) as article_url:\n",
        "      article_text = article_url.read().decode()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6JqjIFpu46v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_text = advanced_preprocess(article_text)\n",
        "sims = sims(model, normalized_text)\n",
        "\n",
        "print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА «{}»\\n'.format(article_text)\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, get_article_text_by_index(index, train_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}